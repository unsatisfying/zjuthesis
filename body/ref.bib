% use `texdoc biblatex` to get help
@www{zjuthesisrules,
    title = {浙江大学本科生毕业论文（设计）编写规则},
    author = {浙江大学本科生院},
    year = {2018},
    url = {http://bksy.zju.edu.cn/attachments/2018-01/01-1517384518-1149149.pdf},
}
@www{tikz,
    title = {tikz宏包},
    author = {Till Tantau},
    year = {2018},
    url = {https://sourceforge.net/projects/pgf/},
}
@www{zjuthesis,
    title = {浙江大学毕业设计/论文模板},
    author = {王子轩},
    year = {2019},
    url = {https://github.com/TheNetAdmin/zjuthesis},
}
@www{zjugradthesisrules,
    title = {浙江大学研究生学位论文编写规则},
    author = {浙江大学研究生院},
    year = {2008},
    url = {http://grs.zju.edu.cn/redir.php?catalog_id=10038&object_id=12877},
}

@www{BYD2023L3,
  author = {{比亚迪}},
  title = {比亚迪获全国首张 {L3} 自动驾驶高快速路测试牌照，全面加速智能化布局},
  url = {https://www.byd.com/cn/news/2023/detail496},
  note = {发布于 2023‑12‑27，首次获得 L3 级自动驾驶测试牌照时间为 2023‑07‑21，地点：深圳市，中国},
  year = {2023},
  langid = {chinese}
}

@www{AppleSiri2025,
  author = {{Apple Inc.}},
  title = {Siri},
  url = {https://www.apple.com/siri/},
  note = {访问日期: 2025‑07‑30, 官方页面介绍 Siri 可用语音控制设备多项操作，并强调隐私安全},
  year = {2025},
  langid = {english}
}

@www{HuaweiXiaoyi2025,
  author = {{Huawei}},
  title = {小艺助手},
  url = {https://xiaoyi.huawei.com/chat/},
  year = {2025}
}

@www{OpenAIChatGPT2022,
  author = {{OpenAI}},
  title = {ChatGPT},
  url = {https://openai.com/zh-Hans-CN/index/chatgpt/},
  year = {2022}
}

@www{opencv-python,
  author = {Bradski, Gary and the OpenCV team},
  title = {opencv-python: OpenCV library for Python},
  year = {2025},
  url = {https://pypi.org/project/opencv-python/}
}

@www{huggingface2024,
  author       = {Hugging Face Inc.},
  title        = {{Hugging Face Hub}: A Platform for Sharing Machine Learning Models, Datasets and Demos},
  howpublished = {\url{https://huggingface.co/}},
  year         = {2026},
  note         = {Accessed: 2024-11-15}
}

@www{modelzoo2024,
  title        = {{Model Zoo}: A Collection of Pre-trained Deep Learning Models},
  author       = {{Various Contributors}},
  howpublished = {\url{https://modelzoo.co/}},
  year         = {2026},
  note         = {Community-maintained platform for model sharing. Accessed: 2024-11-15}
}

@www{tensorflowhub2024,
  title        = {{TensorFlow Hub}: A Repository of Trained Machine Learning Models},
  author       = {{Google Research}},
  howpublished = {\url{https://www.tensorflow.org/hub}},
  year         = {2026},
  note         = {Library for reusable machine learning modules}
}

@inproceedings{cheng2022conflict,
author = {Cheng, Wei and Zhu, Xiangrong and Hu, Wei},
title = {Conflict-Aware Inference of Python Compatible Runtime Environments with Domain Knowledge Graph},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510078},
doi = {10.1145/3510003.3510078},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {451–461},
numpages = {11},
keywords = {configuration management, knowledge graph, python, conflict resolution, dependency solving, runtime environment inference},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{mukherjee2021fixing,
  title={Fixing dependency errors for Python build reproducibility},
  author={Mukherjee, Suchita and Almanza, Abigail and Rubio-Gonz{\'a}lez, Cindy},
  booktitle={Proceedings of the 30th ACM SIGSOFT international symposium on software testing and analysis},
  pages={439--451},
  year={2021}
}

@misc{pipreq,
  title =  {pipreq},
  author = {Jessamyn Smith},
  url = {https://github.com/bndr/pipreqs/},
  year = {2023},
}

@inproceedings{ye2022knowledge,
  title={Knowledge-based environment dependency inference for Python programs},
  author={Ye, Hongjie and Chen, Wei and Dou, Wensheng and Wu, Guoquan and Wei, Jun},
  booktitle={Proceedings of the 44th International Conference on Software Engineering},
  pages={1245--1256},
  year={2022}
}

@article{mahon2025pypitfall,
  title={PyPitfall: Dependency Chaos and Software Supply Chain Vulnerabilities in Python},
  author={Mahon, Jacob and Hou, Chenxi and Yao, Zhihao},
  journal={arXiv preprint arXiv:2507.18075},
  year={2025}
}

@article{alfadel2023empirical,
author = {Alfadel, Mahmoud and Costa, Diego Elias and Shihab, Emad},
title = {Empirical analysis of security vulnerabilities in Python packages},
year = {2023},
issue_date = {May 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-022-10278-4},
doi = {10.1007/s10664-022-10278-4},
abstract = {Software ecosystems play an important role in modern software development, providing an open platform of reusable packages that speed up and facilitate development tasks. However, this level of code reusability supported by software ecosystems also makes the discovery of security vulnerabilities much more difficult, as software systems depend on an increasingly high number of packages. Recently, security vulnerabilities in the npm ecosystem, the ecosystem of Node.js packages, have been studied in the literature. As different software ecosystems embody different programming languages and particularities, we argue that it is also important to study other popular programming languages to build stronger empirical evidence about vulnerabilities in software ecosystems. In this paper, we present an empirical study of 1,396 vulnerability reports affecting 698 Python packages in the Python ecosystem (PyPi). In particular, we study the propagation and life span of security vulnerabilities, accounting for how long they take to be discovered and fixed. In addition, vulnerabilities in packages may affect software projects that depend on them (dependent projects), making them vulnerable too. We study a set of 2,224 GitHub Python projects, to better understand the prevalence of vulnerabilities in their dependencies and how fast it takes to update them. Our findings show that the discovered vulnerabilities in Python packages are increasing over time, and they take more than 3 years to be discovered. A large portion of these vulnerabilities (40.86\%) are only fixed after being publicly announced, giving ample time for attackers exploitation. Moreover, we find that more than half of the dependent projects rely on at least one vulnerable package, taking a considerably long time (7 months) to update to a non-vulnerable version. We find similarities in some characteristics of vulnerabilities in PyPi and npm and divergences that can be attributed to specific PyPi policies. By leveraging our findings, we provide a series of implications that can help the security of software ecosystems by improving the process of discovering, fixing and managing package vulnerabilities.},
journal = {Empirical Softw. Engg.},
month = mar,
numpages = {37},
keywords = {Empirical studies, Vulnerabilities, Packages, PyPi, Python}
}

@ARTICLE{bogaerts2024taxonomy,
  author={Bogaerts, Frédéric C. G. and Ivaki, Naghmeh and Fonseca, José},
  journal={IEEE Open Journal of the Computer Society}, 
  title={A Taxonomy for Python Vulnerabilities}, 
  year={2024},
  volume={5},
  number={},
  pages={368-379},
  keywords={Python;Security;Taxonomy;Codes;Artificial intelligence;Training;Testing;Computing milieux;error handling and recovery;management of computing and information systems;reliability;software engineering;software/software engineering;software quality/SQA;security and protection;testing and debugging},
  doi={10.1109/OJCS.2024.3422686}}

@article{cao2023towards,
author = {Cao, Yulu and Chen, Lin and Ma, Wanwangying and Li, Yanhui and Zhou, Yuming and Wang, Linzhang},
title = {Towards Better Dependency Management: A First Look at Dependency Smells in Python Projects},
year = {2023},
issue_date = {April 2023},
publisher = {IEEE Press},
volume = {49},
number = {4},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2022.3191353},
doi = {10.1109/TSE.2022.3191353},
journal = {IEEE Trans. Softw. Eng.},
month = apr,
pages = {1741–1765},
numpages = {25}
}

@inproceedings{abadi2016tensorflow,
  title={$\{$TensorFlow$\}$: a system for $\{$Large-Scale$\}$ machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th USENIX symposium on operating systems design and implementation (OSDI 16)},
  pages={265--283},
  year={2016}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@INPROCEEDINGS{ladisa2023taxonomy,
  author={Ladisa, Piergiorgio and Plate, Henrik and Martinez, Matias and Barais, Olivier},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)}, 
  title={SoK: Taxonomy of Attacks on Open-Source Software Supply Chains}, 
  year={2023},
  volume={},
  number={},
  pages={1509-1526},
  keywords={Surveys;Visualization;Costs;Taxonomy;Supply chains;Malware;Security;Open Source;Security;Software Supply Chain;Malware;Attack},
  doi={10.1109/SP46215.2023.10179304}}

@article{sonatype2021,
  author = {Sonatype},
  title = {State of the 2021 Software Supply Chain},
  year = {2021},
  journal = {Sonatype Blog},
  url = {https://www.sonatype.com/blog/software-supply-chain-2021},
}

@inproceedings{akhoundali2024morefixes,
  title={MoreFixes: A large-scale dataset of CVE fix commits mined through enhanced repository discovery},
  author={Akhoundali, Jafar and Nouri, Sajad Rahim and Rietveld, Kristian and Gadyatskaya, Olga},
  booktitle={Proceedings of the 20th International Conference on Predictive Models and Data Analytics in Software Engineering},
  pages={42--51},
  year={2024}
}

@www{huggingface-2m-models,
    author = {Francisco Ríos},
    title = {Hugging Face's two million models and counting},
    year = {2025},
    month = dec,
    day = {8},
    url = {https://aiworld.eu/stories/hugging-face-two-million-models},
    note = {Accessed: 2025-12-24},
    publisher = {AI World}
}

@www{jfrog-malicious-huggingface-models,
    author = {{JFrog Security Research Team}},
    title = {Examining Malicious Hugging Face ML Models with Silent Backdoor},
    year = {2025},
    url = {https://research.jfrog.com/examining-malicious-hugging-face-ml-models-with-silent-backdoor/},
    note = {Accessed: 2025-12-24},
    publisher = {JFrog Security Research}
}

@INPROCEEDINGS{ji2017backdoor,
  author={Ji, Yujie and Zhang, Xinyang and Wang, Ting},
  booktitle={2017 IEEE Conference on Communications and Network Security (CNS)}, 
  title={Backdoor attacks against learning systems}, 
  year={2017},
  volume={},
  number={},
  pages={1-9},
  keywords={Feature extraction;Security;Data mining;Skin cancer;Force;Tuning;Conferences},
  doi={10.1109/CNS.2017.8228656}}

  @article{kurakin2016adversarial,
  title={Adversarial machine learning at scale},
  author={Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  journal={arXiv preprint arXiv:1611.01236},
  year={2016}
}

@article{gu2019badnets,
  title={Badnets: Evaluating backdooring attacks on deep neural networks},
  author={Gu, Tianyu and Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={Ieee Access},
  volume={7},
  pages={47230--47244},
  year={2019},
  publisher={IEEE}
}

@article{turner2019label,
  title={Label-consistent backdoor attacks},
  author={Turner, Alexander and Tsipras, Dimitris and Madry, Aleksander},
  journal={arXiv preprint arXiv:1912.02771},
  year={2019}
}

@article{huang2017adversarial,
  title={Adversarial attacks on neural network policies},
  author={Huang, Sandy and Papernot, Nicolas and Goodfellow, Ian and Duan, Yan and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1702.02284},
  year={2017}
}

@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}

@article{yin2024lobam,
  title={LoBAM: LoRA-Based Backdoor Attack on Model Merging},
  author={Yin, Ming and Zhang, Jingyang and Sun, Jingwei and Fang, Minghong and Li, Hai and Chen, Yiran},
  journal={arXiv preprint arXiv:2411.16746},
  year={2024}
}

@article{liu2024lora,
  title={Lora-as-an-attack! piercing llm safety under the share-and-play scenario},
  author={Liu, Hongyi and Liu, Zirui and Tang, Ruixiang and Yuan, Jiayi and Zhong, Shaochen and Chuang, Yu-Neng and Li, Li and Chen, Rui and Hu, Xia},
  journal={arXiv e-prints},
  pages={arXiv--2403},
  year={2024}
}


@article{hua2024malmodel,
title={MalModel: Hiding Malicious Payload in Mobile Deep Learning Models with Black-box Backdoor Attack},
author={Hua, Jiayi and Wang, Kailong and Wang, Meizhen and Bai, Guangdong and Luo, Xiapu and Wang, Haoyu},
journal={arXiv preprint arXiv:2401.02659},
year={2024}
}

@article{hitaj2024trust,
title={Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem},
author={Hitaj, Dorjan and Pagnotta, Giulio and De Gaspari, Fabio and Ruko, Sediola and Hitaj, Briland and Mancini, Luigi V. and Perez-Cruz, Fernando},
journal={arXiv preprint arXiv:2403.03593},
year={2024}
}

@inproceedings{10.1145/3427228.3427268,
author = {Liu, Tao and Liu, Zihao and Liu, Qi and Wen, Wujie and Xu, Wenyao and Li, Ming},
title = {StegoNet: Turn Deep Neural Network into a Stegomalware},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427268},
doi = {10.1145/3427228.3427268},
abstract = {Deep Neural Networks (DNNs) are now presenting human-level performance on many real-world applications, and DNN-based intelligent services are becoming more and more popular across all aspects of our lives. Unfortunately, the ever-increasing DNN service implies a dangerous feature which has not yet been well studied–allowing the marriage of existing malware and DNN model for any pre-defined malicious purpose. In this paper, we comprehensively investigate how to turn DNN into a new breed evasive self-contained stegomalware, namely StegoNet, using model parameter as a novel payload injection channel, with no service quality degradation (i.e. accuracy) and the triggering event connected to the physical world by specified DNN inputs. A series of payload injection techniques which take advantage of a variety of unique neural network natures like complex structure, high error resilience capability and huge parameter size, are developed for both uncompressed models (with model redundancy) and deeply compressed models tailored for resource-limited devices (no model redundancy), including LSB substitution, resilience training, value mapping, and sign-mapping. We also proposed a set of triggering techniques like logits trigger, rank trigger and fine-tuned rank trigger to trigger StegoNet by specific physical events under realistic environment variations. We implement the StegoNet prototype on Nvidia Jetson TX2 testbed. Extensive experimental results and discussions on the evasiveness, integrity of proposed payload injection techniques, and the reliability and sensitivity of the triggering techniques, well demonstrate the feasibility and practicality of StegoNet.},
booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
pages = {928–938},
numpages = {11},
location = {Austin, USA},
series = {ACSAC '20}
}
@article{wang2022evilmodel,
  title={Evilmodel 2.0: bringing neural network models into malware attacks},
  author={Wang, Zhi and Liu, Chaoge and Cui, Xiang and Yin, Jie and Wang, Xutong},
  journal={Computers \& Security},
  volume={120},
  pages={102807},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{wang2021evilmodel,
  title={Evilmodel: hiding malware inside of neural network models},
  author={Wang, Zhi and Liu, Chaoge and Cui, Xiang},
  booktitle={2021 IEEE Symposium on Computers and Communications (ISCC)},
  pages={1--7},
  year={2021},
  organization={IEEE}
}

@www{tensorflow-rce-poc,
    author = {Splinter0},
    title = {Tensorflow Remote Code Execution with Malicious Model},
    year = {2024},
    url = {https://github.com/Splinter0/tensorflow-rce},
    note = {Accessed: 2025-12-26},
    publisher = {GitHub}
}

@www{cert-vul-253266,
    author = {{CERT Coordination Center}},
    title = {Vulnerability Note VU\#253266},
    year = {2025},
    url = {https://kb.cert.org/vuls/id/253266},
    note = {Accessed: 2025-12-26},
    publisher = {CERT/CC}
}

@www{hackernews-sleepy-pickle,
    author = {{The Hacker News}},
    title = {New Attack Technique 'Sleepy Pickle' Targets Machine Learning Models},
    year = {2024},
    month = jun,
    url = {https://thehackernews.com/2024/06/new-attack-technique-sleepy-pickle.html},
    note = {Accessed: 2025-12-26},
    publisher = {The Hacker News}
}

@www{github-pickle-attacks,
    author = {pjcampbe11},
    title = {Pickle-File-Attacks},
    year = {2024},
    url = {https://github.com/pjcampbe11/Pickle-File-Attacks},
    note = {Accessed: 2025-12-26},
    publisher = {GitHub}
}

@www{trailofbits-pickle-attacks,
    author = {{Trail of Bits}},
    title = {Exploiting ML Models with Pickle File Attacks (Part 1)},
    year = {2024},
    month = jun,
    day = {11},
    url = {https://blog.trailofbits.com/2024/06/11/exploiting-ml-models-with-pickle-file-attacks-part-1/},
    note = {Accessed: 2025-12-26},
    publisher = {Trail of Bits}
}

@www{python3123pickletools,
title={pickletools — Tools for pickle developers},
author={{Python Software Foundation}},
howpublished={\url{https://docs.python.org/3/library/pickletools.html}},
year={2023},
note={Accessed: 2024-06-06}
}

@www{fickling_defcon_2021,
title        = {Fickling @ DEFCON AI Village 2021},
author       = {Trail of Bits},
howpublished = {Online},
year         = {2021},
note         = {Available: \url{https://github.com/trailofbits/fickling}}
}
@www{picklescan,
title        = {Python Pickle Malware Scanner},
author       = {Mmaitre314},
howpublished = {Online},
year         = {2024},
note         = {Available: \url{https://pypi.org/project/picklescan/}}
}

@www{modelscan_github_2024,
author = {protectai},
title = {ModelScan: Protection against Model Serialization Attacks},
year = {2024},
publisher = {GitHub},
journal = {GitHub repository},
howpublished = {\url{https://github.com/protectai/modelscan}}
}

@www{utoronto-gpuhammer,
    author = {{University of Toronto}},
    title = {How three U of T researchers discovered a GPU vulnerability that threatened AI models},
    year = {2024},
    url = {https://www.utoronto.ca/news/how-three-u-t-researchers-discovered-gpu-vulnerability-threatened-ai-models},
    note = {Accessed: 2025-12-26},
    publisher = {University of Toronto}
}

@www{nvidia-driver-cve-1,
    author = {{NVIDIA Corporation}},
    title = {Product Security},
    year = {2025},
    url = {https://nvidia.custhelp.com/app/answers/detail/a_id/5703},
    note = {Accessed: 2025-12-26},
    publisher = {NVIDIA}
}
@www{nvidia-driver-cve-2,
    author = {{NVIDIA Corporation}},
    title = {Product Security},
    year = {2025},
    url = {https://nvidia.custhelp.com/app/answers/detail/a_id/5630},
    note = {Accessed: 2025-12-26},
    publisher = {NVIDIA}
}
@www{nvidia-driver-cve-3,
    author = {{NVIDIA Corporation}},
    title = {Product Security},
    year = {2025},
    url = {https://nvidia.custhelp.com/app/answers/detail/a_id/5670},
    note = {Accessed: 2025-12-26},
    publisher = {NVIDIA}
}

@www{wiz-nvidia-ai-vulnerability,
    author = {{Wiz Research}},
    title = {NVIDIA AI Vulnerability CVE-2025-23266 (NVIDIAScape)},
    year = {2025},
    month = jul,
    url = {https://www.wiz.io/blog/nvidia-ai-vulnerability-cve-2025-23266-nvidiascape},
    note = {Accessed: 2025-12-26},
    publisher = {Wiz}
}

@www{cve-2025-23346,
    author = {{National Institute of Standards and Technology}},
    title = {CVE-2025-23346},
    year = {2025},
    month = sep,
    day = {24},
    url = {https://nvd.nist.gov/vuln/detail/CVE-2025-23346},
    note = {Accessed: 2025-12-26},
    publisher = {NIST}
}

@www{nvidia-container-toolkit-security-bulletin,
    author = {{NVIDIA Corporation}},
    title = {Security Bulletin: NVIDIA Container Toolkit - July 2025},
    year = {2025},
    month = jul,
    url = {https://nvidia.custhelp.com/app/answers/detail/a_id/5659/~/security-bulletin%3A-nvidia-container-toolkit---july-2025},
    note = {Accessed: 2025-12-26},
    publisher = {NVIDIA}
}

@www{aimonks-nvidiascape-vulnerability,
    author = {{AIMonks}},
    title = {The NVIDIAScape Vulnerability: A Wake-Up Call for Closed-Source AI Infrastructure},
    year = {2025},
    url = {https://medium.com/aimonks/the-nvidiascape-vulnerability-a-wake-up-call-for-closed-source-ai-infrastructure-b07a745bdac2},
    note = {Accessed: 2025-12-26},
    publisher = {Medium}
}

@www{nvidia-vulnerability-analysis-container-security,
    author = {{NVIDIA Corporation}},
    title = {Vulnerability Analysis for Container Security},
    year = {2024},
    url = {https://build.nvidia.com/nvidia/vulnerability-analysis-for-container-security},
    note = {Accessed: 2025-12-26},
    publisher = {NVIDIA}
}

@www{cve-2024-53870,
    author = {{MITRE Corporation}},
    title = {CVE-2024-53870},
    year = {2024},
    url = {https://www.cve.org/CVERecord?id=CVE-2024-53870},
    note = {Accessed: 2025-12-26},
    publisher = {CVE.org}
}

@www{cve-2024-53871,
    author = {{MITRE Corporation}},
    title = {CVE-2024-53871},
    year = {2024},
    url = {https://www.cve.org/CVERecord?id=CVE-2024-53871},
    note = {Accessed: 2025-12-26},
    publisher = {CVE.org}
}

@www{cve-2024-53872,
    author = {{MITRE Corporation}},
    title = {CVE-2024-53872},
    year = {2024},
    url = {https://www.cve.org/CVERecord?id=CVE-2024-53872},
    note = {Accessed: 2025-12-26},
    publisher = {CVE.org}
}

@article{jia2019dissecting,
  title={Dissecting the nvidia turing t4 gpu via microbenchmarking},
  author={Jia, Zhe and Maggioni, Marco and Smith, Jeffrey and Scarpazza, Daniele Paolo},
  journal={arXiv preprint arXiv:1903.07486},
  year={2019}
}

@article{jia2018dissecting,
  title={Dissecting the NVIDIA volta GPU architecture via microbenchmarking},
  author={Jia, Zhe and Maggioni, Marco and Staiger, Benjamin and Scarpazza, Daniele P},
  journal={arXiv preprint arXiv:1804.06826},
  year={2018}
}

@inproceedings{abdelkhalik2022demystifying,
  title={Demystifying the nvidia ampere architecture through microbenchmarking and instruction-level analysis},
  author={Abdelkhalik, Hamdy and Arafa, Yehia and Santhi, Nandakishore and Badawy, Abdel-Hameed A},
  booktitle={2022 IEEE High Performance Extreme Computing Conference (HPEC)},
  pages={1--8},
  year={2022},
  organization={Ieee}
}

@article{jarmusch2025dissecting,
  title={Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks},
  author={Jarmusch, Aaron and Graddon, Nathan and Chandrasekaran, Sunita},
  journal={arXiv preprint arXiv:2507.10789},
  year={2025}
}

@article{luo2025dissecting,
  title={Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and Multiple Level Analysis},
  author={Luo, Weile and Fan, Ruibo and Li, Zeyu and Du, Dayou and Liu, Hongyuan and Wang, Qiang and Chu, Xiaowen},
  journal={arXiv preprint arXiv:2501.12084},
  year={2025}
}

@inproceedings{naghibijouybari2018rendered,
  title={Rendered insecure: GPU side channel attacks are practical},
  author={Naghibijouybari, Hoda and Neupane, Ajaya and Qian, Zhiyun and Abu-Ghazaleh, Nael},
  booktitle={Proceedings of the 2018 ACM SIGSAC conference on computer and communications security},
  pages={2139--2153},
  year={2018}
}

@inproceedings{zhang2023t,
  title={T unne L s for B ootlegging: Fully Reverse-Engineering GPU TLBs for Challenging Isolation Guarantees of NVIDIA MIG},
  author={Zhang, Zhenkai and Allen, Tyler and Yao, Fan and Gao, Xing and Ge, Rong},
  booktitle={Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
  pages={960--974},
  year={2023}
}

@inproceedings{nayak2021mis,
  title={(mis) managed: A novel tlb-based covert channel on gpus},
  author={Nayak, Ajay and B, Pratheek and Ganapathy, Vinod and Basu, Arkaprava},
  booktitle={Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security},
  pages={872--885},
  year={2021}
}

@inproceedings{dutta2021leakybuddies,
  title={Leaky buddies: Cross-component covert channels on integrated cpu-gpu systems},
  author={Dutta, Sankha Baran and Naghibijouybari, Hoda and Abu-Ghazaleh, Nael and Marquez, Andres and Barker, Kevin},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={972--984},
  year={2021},
  organization={IEEE}
}

@inproceedings {yanan:profit_for_fun,
author = {Yanan Guo and Zhenkai Zhang and Jun Yang},
title = {{GPU} Memory Exploitation for Fun and Profit},
booktitle = {33rd USENIX Security Symposium (USENIX Security 24)},
year = {2024},
isbn = {978-1-939133-44-1},
address = {Philadelphia, PA},
pages = {4033--4050},
url = {https://www.usenix.org/conference/usenixsecurity24/presentation/guo-yanan},
publisher = {USENIX Association},
month = aug
}

@article{mittal2018survey,
  title={A Survey of Techniques for Improving Security of GPUs},
  author={Mittal, Sparsh and Abhinaya, S. B. and Reddy, Manish and Ali, Irfan},
  journal={Journal of Hardware and Systems Security},
  volume={2},
  number={3},
  pages={266--285},
  year={2018},
  publisher={Springer}
}

@article{miele2016buffer,
  title={Buffer Overflow Vulnerabilities in CUDA: A Preliminary Analysis},
  author={Miele, Andrea},
  journal={Journal of Computer Virology and Hacking Techniques},
  volume={12},
  number={2},
  pages={113--120},
  year={2016},
  publisher={Springer}
}

@article{park2020mind,
  title={Mind Control Attack: Undermining Deep Learning with GPU Memory Exploitation},
  author={Park, Sang-Ok and Kwon, Ohmin and Kim, Yonggon and Cha, Sang Kil and Yoon, Hyunsoo},
  journal={Computers \& Security},
  volume={102},
  pages={102115},
  year={2020},
  publisher={Elsevier}
}

@article{tyler2024leftoverlocals,
  title={LeftoverLocals: Listening to LLM Responses Through Leaked GPU Local Memory},
  author={Sorensen, Tyler and Khlaaf, Heidy},
  journal={arXiv preprint arXiv:2401.16603},
  year={2024}
}

@inproceedings{returning_exploits2025,
author = {Roels, Jonas and Jacobs, Adriaan and Volckaert, Stijn},
title = {CUDA, Woulda, Shoulda: Returning Exploits in a SASS-y World},
year = {2025},
isbn = {9798400715631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722041.3723099},
doi = {10.1145/3722041.3723099},
abstract = {The rising popularity of Graphics Processing Units (GPUs) has made them an attractive target for attackers looking to steal Intellectual Property (IP) such as ML models or disrupt the operation of heterogeneous computing systems. However, defending against GPU attacks is anything but trivial since the inner workings of these-often proprietary-devices are still poorly understood. Preliminary work demonstrates a worrying similarity to the attack surface of the CPU domain, particularly concerning the memory unsafety of device-side code. We corroborate these worrying findings by constructing the first rigorous experimental analysis of input-triggered, ROP-based exploits entirely within device-side NVIDIA CUDA code. We repurposed known CPU-based code-reuse attack techniques to unlock previously unusable gadgets in this code and demonstrate that the gadget set is Turing-complete, enabling attackers to perform arbitrary computations. We conclude that ROP attacks on GPUs are feasible and more potent than previously thought.Following this discovery, we evaluate the strength of current device-side mitigations, such as stack canaries and Address Space Layout Randomization (ASLR). Given the lack of more powerful protection mechanisms, these basic security measures play a crucial role in GPU security. However, we find them even less secure than their CPU counterparts. Our findings indicate that the GPU domain urgently needs robust protection mechanisms that fit the unique GPU architectures and address the flaws in existing systems.},
booktitle = {Proceedings of the 18th European Workshop on Systems Security},
pages = {40–48},
numpages = {9},
keywords = {CUDA, Code-Reuse Attacks, GPU, Graphics Processing Unit, Memory Safety, NVIDIA, ROP, Return-Oriented Programming},
location = {Rotterdam, Netherlands},
series = {EuroSec'25}
}