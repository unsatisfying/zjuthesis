\chapter{绪论}
\section{研究背景及意义}

随着人工智能 (Artificial Intelligence, AI) 技术的迅猛发展，AI 正在加速融入人类社会的各个领域，并逐渐成为推动社会进步与产业升级的重要引擎。在日常生活中，AI技术已广泛应用于自动驾驶、智能助手、自然语言处理等关键场景。
例如在自动驾驶领域，比亚迪推出的“天神之眼”高阶智能驾驶辅助系统，能够实现全场景的感知与控制辅助功能，为用户提供更加安全、高效的出行体验~\cite{BYD2023L3}；
在智能助手方面，苹果公司的“Siri助手”与华为的“小艺助手”能够执行语音指令，完成文件操作、应用启动等任务，显著提升了人机交互的便捷性~\cite{AppleSiri2025,HuaweiXiaoyi2025}；
在自然语言生成领域，OpenAI 于2022年发布的 \code{ChatGPT} 引发广泛关注，标志着以大参数语言模型 (Large Language Model, LLM) 为代表的生成式 AI 技术进入高速发展阶段~\cite{OpenAIChatGPT2022}。
AI 的广泛应用不仅加速了社会向数字化、信息化与智能化的转型，也成为衡量国家科技竞争力的重要标志。

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figure/AI系统框架图.png}
    \caption{\label{fig:AI_system}AI系统架构图}
\end{figure}

\textbf{AI系统的分层架构。}
随着AI技术成体系地持续演化，目前业界研究重点已逐步从单一模型的性能和结构优化，扩展至模型在真实系统中的集成、部署与运行效率等更为系统性的问题。事实上，在复杂应用环境中，AI 模型往往被嵌入到一个多层次、异构化的AI系统中，形成从前端应用到后端算力硬件支持的一体化处理链。所谓 AI 系统，是指由 AI 模型、模型管理软件、运行时环境支持的AI框架以及底层硬件资源协同构建而成的综合性技术体系，其核心任务是对图像、语音、文本等输入数据进行智能化分析，并输出相应的决策结果或交互反馈。如\autoref{fig:AI_system}所示，现代 AI 系统通常由三层组成：软件应用层、模型框架层和硬件加速层，三者之间层层依赖、密切协同，共同构成支撑 AI 服务运行的完整技术栈。

软件应用层处于 AI 系统的最上层，直接面向终端用户，负责构建各类 AI 模型驱动的应用程序。在该层中，开发者主要使用 Python 语言调用预训练的AI模型，同时结合 Java、C++ 等高级编程语言实现定制化的业务逻辑和系统功能，例如自动驾驶、人脸识别、智能助手、文字生成等智能服务。这些应用可以通过嵌入式部署或远程服务调用的方式对接模型推理模块，从而灵活适配本地部署或云端服务等不同运行环境。

模型框架层位于AI系统的中间层，是连接上层应用与下层硬件的核心支撑组件，承担模型训练、推理与部署的功能。在这一层，开发者通常依赖 TensorFlow、PyTorch 等主流深度学习框架~\cite{abadi2016tensorflow,paszke2019pytorch}，通过其提供的高层 API（多以 Python 形式暴露）定义模型结构，并调用由 C++ 或通用并行计算语言实现的底层算子，高效完成模型计算与参数优化。此外，受限于训练过程对算力资源和高质量标注数据的高昂需求，开发者往往从开源模型平台引入预训练模型，并通过迁移学习或微调的方式实现定制化能力。这一实践在显著提升开发效率和迭代速度的同时，也使模型框架层成为 AI 系统中高度依赖外部资源的关键环节。

硬件加速层位于AI系统的最底层，为 AI 模型的算子运算提供实际的运行平台和算力保障。鉴于深度学习模型普遍具有高度并行的计算特性，单纯依赖 CPU 已难以满足性能需求，因此该层通常采用 NVIDIA GPU、Intel NPU、Google TPU 等专用加速硬件。同时，操作系统之上还运行着各类支持通用并行计算的平台，如 CUDA、OpenCL 等。这些平台通过底层驱动与编译器将AI框架中的算子编译为GPU或TPU等硬件指令，并由调度器分配至合适的计算单元，从而实现对模型计算过程的高效加速。

\textbf{AI系统的供应链。}
在 AI 系统中这种多层异构架构显然极大地帮助开发者提升了开发效率和模型运行效率，然而系统的多层级复杂性也引入了高度复杂的供应链关系，使系统整体暴露于跨层级、跨组件的安全风险之中。在这种分层结构下，每一层均依赖大量第三方库、开源框架或底层驱动组件。当某一层的组件受到攻击或被植入恶意行为时，由于下层为上层提供运行支撑、上层对下层进行功能抽象，这种威胁极易沿着依赖链条向上传播，最终影响整个 AI 系统的安全性与稳定性，造成信息泄露、资产损失甚至服务中断等严重后果。

在软件应用层，开发者为了提升开发效率、减少重复实现，通常会引入大量开源第三方软件包。例如在 Python 生态中，图像处理相关的 AI 应用往往依赖 \code{opencv-python} 库~\cite{opencv-python}，该库提供了丰富且高效的图像处理 API，能够在处理图像时采用高效的算法进行增强、还原、除噪。然而这种对第三方依赖的高度信任也构成了显著的供应链风险，一旦依赖包本身或其间接依赖被恶意投毒，或依赖包包含尚未修复的安全漏洞，恶意代码便可能在模型部署或运行阶段被触发，从而破坏整个 AI 系统的安全边界。

在模型框架层，从头开始训练模型的需要大量显卡算力的硬件支持，以及人工标注的数据集的昂贵成本，因此开发者往往选择基于现有预训练模型进行二次开发，修改模型结构或者对其参数进行微调。这些预训练开源模型广泛来源于 HuggingFace、Model Zoo、TensorFlow Hub 等开源模型平台~\cite{huggingface2024,modelzoo2024,tensorflowhub2024}。然而，此类模型来源复杂，且多以二进制格式分发，其内部结构与执行行为对使用者而言往往不可完全验证。一旦模型中被植入恶意后门或隐蔽的可执行逻辑，便可能在推理过程中触发参数篡改、敏感信息泄露，甚至实现任意代码执行，对 AI 系统构成严重威胁。

在硬件加速层中，AI 系统的运行往往使用于不同的加速平台，这些加速平台都依赖底层驱动程序、编译器和固件将AI 算子映射至具体硬件执行逻辑。然而，这些底层组件通常由硬件厂商封闭实现，缺乏透明性，其内部的内存管理机制、计算单元调度方式等细节对用户不可见。一旦这些驱动或固件中存在安全漏洞，或者没有实现特定的安全防护机制，攻击者便可能通过精心构造的模型输入或算子参数触发底层缓冲区溢出，进一步导致权限提升或敏感信息泄露。

综上所述，AI 系统的安全问题已不再局限于单一模型或单一组件，而是深度嵌入于其跨层级、跨组件的复杂供应链之中。因此，构建可信且安全的 AI 系统，必须从供应链全生命周期的角度出发，对各层依赖关系、潜在威胁与防护机制进行系统性分析与设计。

\section{研究现状与目标}
在 AI 系统日益复杂化的背景下，AI 供应链安全问题已逐步受到研究界与工业界的高度关注。随着 AI 应用从单一模型扩展为由多层组件协同构成的复杂系统，其安全性也愈发依赖于不同层级组件之间的依赖关系及每一层独有的供应链机制。

\textbf{AI系统软件应用层供应链研究现状。}
Python 作为 AI 软件开发中最为主流的编程语言之一，围绕其软件应用层的供应链安全问题，也层出不穷，根据 Sonatype 自2019年以来的多年年度报告，不仅开源软件包的数量在逐年激增，恶意软件包的数量也随之层出不穷，截至2024年，Sonatype 组织已经发现超过704 102个恶意的开源软件包~\cite{sonatype2021}。同时报告还指出 CVE 数量也持续呈指数级增长，开发者却无法跟上这样爆炸级的漏洞增长数，无法保证漏洞能够被及时修复。有相关研究表明，部分漏洞在开源软件包中存在的时间甚至长达3年以上未修复~\cite{akhoundali2024morefixes}。高风险的开源软件包不仅会对 AI 软件的开发造成影响，甚至能对整个 AI 系统造成威胁。

目前已有大量研究从开源依赖管理、软件包漏洞以及运行时环境风险等方面展开深入分析。
Cheng 等人提出了 PyCRE 框架，采用静态分析方法修复 Python 供应链中存在的错误依赖问题。其核心思路是通过源码分析与抽象语法树技术 (Abstract Resource Tree， AST) 提取模块之间的依赖关系，并结合软件包配置文件构建依赖图，从而判断依赖图中的依赖项是否存在缺失和冲突，进而修复依赖冲突和版本不一致等问题，以避免因依赖错误导致的 AI 软件部署失败~\cite{cheng2022conflict}。Mukherjee 等人提出了 PyDFix 框架，该框架通过在部署阶段收集运行时的控制台信息，判断安装过程中具体是哪些软件包出现错误，以及错误类型是依赖缺失还是版本不一致，并基于这些错误信息实现对依赖冲突的动态检测与修复~\cite{mukherjee2021fixing}。此外，Pipreq 作为一种静态依赖生成工具，它可以通过自动化地分析 Python 项目中\code{import}语句引入了哪些模块，再通过一个一对一的模块与软件包名的映射，来判断该项目需要哪些软件包，从而能够自动从项目源码中推导出所需的依赖列表，用于生成标准化的\code{requirements.txt}配置文件~\cite{pipreq}。在进一步扩展依赖修复范围方面，Ye 等人提出了 PyEGo 框架，该框架不仅关注软件包层面的依赖问题，还同时考虑系统环境依赖以及 Python 解释器版本兼容性，从而提升整体部署过程的可复现性与鲁棒性~\cite{ye2022knowledge}。此外，Cao 等人提出了 PyDC 框架，针对由于 Python 软件依赖配置错误引发的 Dependency Smell 问题展开研究，系统分析了此类问题的普遍性、成因及其演化过程。

除依赖关系修复外，针对 Python 生态中软件漏洞的分析同样是软件应用层供应链研究的重要方向之一。由于 AI 软件通常依赖大量的核心 AI 组件包和其他开源软件包，这些关键依赖项中潜在的漏洞也是影响 AI 系统安全性的重要因素之一。Mahon 等人提出了 PyPitfall 工具，从整体视角系统分析了 PyPI 生态中的依赖结构及漏洞传播关系，揭示了直接依赖与传递依赖在系统漏洞暴露风险中的显著影响~\cite{mahon2025pypitfall}。Alfadel 等人通过对698个Python包的1396条漏洞报告进行实证分析，发现Python软件包的漏洞数量呈上升趋势，且部分漏洞在被发现前的生命周期超过三年~\cite{alfadel2023empirical}。
在更宏观的层面，Ladisa 等人对开源软件供应链的攻击实现了一个系统的分类，该分类独立于特定的编程语言或生态系统，并覆盖了从代码贡献到软件包分发的所有供应链阶段。其以攻击树的形式刻画了 107 种不同的攻击向量，并将其与 94 起真实世界事件及 33 类缓解措施进行映射~\cite{ladisa2023taxonomy}。类似地，Bogaerts等人则更专注于Python语言，构建了包含1026个已公开Python漏洞的数据库，并提取了对应的补丁与易受攻击代码，为后续漏洞检测与修复研究提供数据基础~\cite{bogaerts2024taxonomy}。

综上所述，现有研究在 AI 软件应用层已提出诸多有效工具和框架，可以用于自动修复 AI 项目中常见的依赖配置错误、漏洞风险检测、软件包部署的错误等问题，从而提升 AI 软件包的稳定性和安全性。然而，现有工作大多聚焦于已知漏洞或显式依赖关系分析，并且通常都是以软件包为分析粒度，对更细粒度的模块级行为关注度较少，同时也尚未深入探讨供应链机制本身如何被恶意利用的问题。

\textbf{AI系统模型框架层供应链研究现状。}
在 AI 系统的模型框架层，研究者逐渐意识到预训练模型的本身及其其所依赖的运行框架和算子在 AI 供应链中的关键地位。近年来，开源模型库中的模型数量呈爆炸式增长。以 Hugging Face 为例，仅在 2022 年至 2025 年期间，该平台上累计发布的开源模型数量已超过 200 万个~\cite{huggingface-2m-models}。如此庞大的模型规模在显著降低模型获取与复用成本的同时，也为恶意模型的传播提供了现实土壤。已有公开报告表明，开源模型库正逐步成为攻击者投放恶意载荷的新型渠道。JFrog 于 2024 年 2 月发布的分析报告指出，其在 Hugging Face 平台上发现了超过 100 个恶意模型，涉及 TensorFlow、PyTorch 等多个主流深度学习框架。这些模型在加载或推理阶段可触发反向 shell、任意文件读写、启动特定程序以及代码执行等恶意行为~\cite{jfrog-malicious-huggingface-models}。相较于传统的软件包投毒攻击，模型与框架层面的攻击更贴近模型的实际执行路径，能够自然嵌入正常的模型加载与推理流程中，因而通常具备更强的隐蔽性和更高的潜在危害性。

围绕模型框架层的安全风险，现有研究已从多个角度展开系统性探索，相关工作大体可归纳为恶意模型行为分析、模型安全检测机制以及模型框架层漏洞挖掘等方向。从攻击目标与实现方式的角度看，模型层面的恶意逻辑注入主要可以划分为两类。

第一类是传统机器学习语境下的恶意模型，其核心目标在于操纵模型的预测或决策结果，而非直接执行系统级恶意行为。例如，攻击者可通过精心设计的训练过程，使智能驾驶模型在特定条件下将红灯错误识别为绿灯，从而间接诱发交通事故。这类攻击主要关注模型推理行为本身的安全性，对系统执行环境的影响通常是间接的。代表性研究包括后门攻击，即在训练阶段向模型中植入隐蔽触发器，使模型在正常输入下表现正常，而在触发条件出现时输出攻击者预期结果~\cite{ji2017backdoor, gu2019badnets, turner2019label}；以及对抗样本攻击，通过对输入样本施加微小扰动诱导模型产生错误分类~\cite{kurakin2016adversarial, huang2017adversarial, madry2017towards}。近年来，随着大参数模型高效微调技术的发展，研究者进一步发现，可利用 LoRA 等轻量化微调机制在不显著影响模型整体性能的前提下植入恶意触发逻辑，从而实现更加隐蔽的攻击~\cite{yin2024lobam, liu2024lora}。

第二类则是将 AI 模型本身作为恶意逻辑载体的攻击方式。在这一语境下，模型不再仅用于产生错误预测结果，而是被直接用于承载、隐藏并触发恶意软件或恶意代码，从而对运行模型的系统环境造成实质性威胁。现有研究表明，此类攻击主要通过以下三种方式实现。其一，攻击者将恶意软件或恶意逻辑嵌入模型的二进制参数或特定层次结构中，并在模型运行阶段对恶意载荷进行重组与触发。Hua 等人提出的 Malmodel 技术，将恶意模型嵌入 TensorFlow Lite 模型的层数、覆盖率等参数中，并利用 Java 反射机制主动触发~\cite{hua2024malmodel}。Hitaj 等人提出的 MaleficNet，利用扩频信道编码结合纠错技术，将恶意负载注入深度学习网络参数中~\cite{hitaj2024trust}。类似地，其他工作如 Evilmodel 1.0、Evilmodel 2.0 以及 StegoNet，则采用最低有效位 (Least Significant Bit，LSB) 隐写术将恶意软件隐藏于模型权重中~\cite{wang2021evilmodel, wang2022evilmodel, 10.1145/3427228.3427268}。其二，攻击者将恶意逻辑直接嵌入模型的 lambda 层中。这类攻击主要适用于支持 lambda 层的模型框架（如 TensorFlow），通过在模型执行过程中触发任意代码执行实现攻击。然而，该方式通常较易被检测，因为仅需检查模型中是否存在 lambda 层并分析其逻辑即可识别异常行为~\cite{tensorflow-rce-poc,cert-vul-253266}。其三，也是目前最为普遍的一类方式，是利用 pickle 等不安全的模型序列化格式，将恶意逻辑嵌入模型文件中，并在模型反序列化过程中触发代码执行~\cite{hackernews-sleepy-pickle, github-pickle-attacks, trailofbits-pickle-attacks}。针对这一威胁，工业界已提出多种检测与分析工具。例如，Pickletools 可对 pickle 格式的模型文件进行反序列化分析，从而识别潜在的恶意函数调用~\cite{python3123pickletools}；Fickling 提供了对 Python pickle 对象的反编译、静态分析和字节码重写能力，既可用于检测嵌入 PyTorch 模型的恶意行为，也可被用于构造攻击载荷~\cite{fickling_defcon_2021}；Picklescan 同样支持对基于 pickle 的恶意 PyTorch 模型进行检测~\cite{picklescan}。目前，业界较为先进的模型检测工具包括 Protect AI 公司推出的 ModelScan，该工具能够识别包括基于 pickle 的恶意模型和 TensorFlow lambda 层攻击在内的多种模型级恶意行为~\cite{modelscan_github_2024}。

综上所述，现有研究已从多个角度揭示了模型框架层在 AI 系统供应链中面临的安全风险，充分地证明了模型本身可以被用作攻击载体。然而，这些工作大多将风险归因于恶意模型本身或不安全的序列化机制，从而将模型框架层的安全边界界定在模型层面，这是不完备的，事实上模型框架层自身和为模型框架提供的算子层面的攻击仍未被充分研究。


\textbf{AI系统硬件加速层供应链研究现状。}
在硬件加速层，AI 系统高度依赖 GPU、NPU 等专用计算设备以满足大规模并行计算与高性能推理需求，其底层供应链通常由计算加速硬件、设备驱动、运行时库以及 CUDA、OpenCL 等编程框架共同构成。随着 GPU 架构与配套软件栈复杂度的持续提升，相关供应链组件逐渐暴露出新的安全风险，使得硬件加速层在 AI 系统中不再仅是被动的计算执行单元，而演变为潜在的重要攻击入口。

随着 GPU 架构与配套软件栈复杂度的持续提升，相关供应链组件逐渐暴露出新的安全风险，使得硬件加速层在 AI 系统中不再仅是被动的计算执行单元，而演变为潜在的重要攻击入口。Saileshwar 等人首次将 Rowhammer 类硬件攻击扩展至 GPU 平台，提出了 GPUHammer 攻击方法，利用 GPU 的高并行特性在显存中诱发比特翻转，从而显著破坏深度学习模型参数的完整性，甚至仅通过翻转单个模型权重比特即可导致模型准确率出现灾难性下降~\cite{utoronto-gpuhammer}。该工作表明，即便不直接攻击模型代码或框架逻辑，底层硬件的不可靠性本身亦可能成为影响 AI 系统可信性的关键因素。

除硬件本体外，围绕 GPU 构建的配套软件同样构成硬件加速层供应链中的重要组成部分，并已被多次证实存在安全隐患。已有公开漏洞表明，NVIDIA GPU 驱动中存在可被利用的高危漏洞，攻击者可借此实现权限提升或非法内存访问~\cite{nvidia-driver-cve-2, nvidia-driver-cve-1, nvidia-driver-cve-3}。与此同时，面向 AI 场景广泛部署的 NVIDIA GPU 容器生态亦被发现存在配置缺陷与隔离失效问题，可能引发跨容器攻击或敏感数据泄漏~\cite{nvidia-vulnerability-analysis-container-security, aimonks-nvidiascape-vulnerability, nvidia-container-toolkit-security-bulletin, wiz-nvidia-ai-vulnerability}。此外，GPU 编译器及相关开发工具链同样曾被披露存在多项安全漏洞，这进一步扩大了硬件加速层在 AI 供应链中的攻击面~\cite{cve-2024-53870, cve-2024-53871, cve-2024-53872}。更为严峻的是，上述驱动、容器与编译器等关键供应链组件多处于闭源或半开源状态，用户与研究者难以对其内部实现进行独立审计，使漏洞发现与修复高度依赖厂商响应，一旦攻击者率先掌握可被稳定利用的漏洞，便可能借助硬件加速层对上层 AI 框架与应用产生连锁影响。

从技术研究角度看，现有国内外学术工作主要从 GPU 架构分析与漏洞利用两个方面对硬件加速层展开系统性研究。在 GPU 架构分析方面，研究者通过微架构测试与逆向工程方法，对不透明或半透明的 GPU 内部实现进行了深入探索。Jia 等人率先对 NVIDIA Volta 架构 GPU 的缓存层次结构与访存机制进行了系统分析~\cite{jia2018dissecting}，随后又扩展至 Turing 架构~\cite{jia2019dissecting}。此后，多项工作采用类似方法对 NVIDIA 不同代 GPU 架构进行逆向分析，旨在理解其内部设计与安全边界~\cite{abdelkhalik2022demystifying, jarmusch2025dissecting, luo2025dissecting}。

在漏洞利用方面，针对 GPU 的攻击研究主要集中于侧信道 (Side-channel Attacks) 与隐蔽信道攻击 (Covert Channel Attack)。Naghibijouybari 等人首次证明，基于 OpenGL 或 CUDA 的间谍程序可以通过 GPU 侧信道提取网页指纹、用户交互行为，甚至恢复其他 CUDA 应用中神经网络模型的内部参数~\cite{naghibijouybari2018rendered}。Zhang 等人进一步逆向了 Ampere 架构 GPU 的页表实现细节和多级缓存 (Cache) 的实现细节，并指出在多实例 GPU 特性 (Multi-Instance GPU, MIG) 场景下，由于 L3 Cache 共享机制仍然存在跨实例侧信道风险~\cite{zhang2023t}。Nayak 等人利用统一虚拟内存 (Unified Virtual Memory, UVM) 和快表 (Translation Lookaside Buffer, TLB) 机制，在 GPU 上构建隐蔽信道，实现了对 GPU 加速数据库应用数据的泄漏~\cite{nayak2021mis}。此外，Dutta 等人利用 GPU 与 CPU 之间共享缓存与总线的特性，在 Intel 平台上构建了高带宽隐蔽信道，进一步拓展了跨硬件组件攻击的可能性~\cite{dutta2021leakybuddies}。

在内存漏洞分析方面，已有研究揭示了 GPU 内存管理机制中存在的多种安全隐患。Guo 等人对 NVIDIA GPU 上的越界访问 (Out Of Bound, OOB) 漏洞进行了系统性分析，证实了 GPU 上 OOB BUG 利用的可能性，他们还对 GPU 栈内存布局进行了逆向工程~\cite{yanan:profit_for_fun}。Mittal 等人对 GPU 漏洞进行了全面综述，从数据泄露、侧信道与隐蔽信道等角度对攻击模式进行了系统分类~\cite{mittal2018survey}。Miele 等人利用 GPU 上的栈溢出漏洞劫持函数指针，并分析了在 GPU 环境中实施返回导向编程攻击 (Return-Oriented Programming, ROP) 的可行性~\cite{miele2016buffer}。此外，Park 等人提出的 \code{Mind Control} 攻击通过操纵 GPU 设备内存并利用固定 CUDA 库地址干扰深度学习系统推理过程~\cite{park2020mind}；Sorensen 等人提出的 \code{LeftoverLocals} 攻击，利用未初始化的 GPU 局部内存实现跨进程或跨容器的数据恢复，他们的研究恢复了 Apple、Qualcomm 和 AMD 等厂商的 GPU 上的局部内存数据，对其他用户的交互式大语言模型会话进行窃听~\cite{tyler2024leftoverlocals}。Roels 等人进一步研究了 GPU 内存中的 ROP 小组件，并提出了绕过 NVIDIA 将返回地址存储在寄存器中的防御机制的组合式攻击方法~\cite{returning_exploits2025}。

综上所述，现有研究从硬件架构、配套驱动和工具链软件，以及漏洞利用等多个维度系统揭示了硬件加速层的供应链在安全性方面的潜在风险。然而这些工作大多聚焦于单点漏洞、特定攻击技术或底层实现缺陷，并且仅仅将安全边界界定于 GPU 或者 NPU 等硬件加速设备，将其设为孤立的攻击目标，并未深入分析跨设备的安全性，例如是否可以从 GPU 侧威胁到 CPU 侧的安全性，再由此通过框架与运行时接口向上层 AI 系统传导安全影响。

\section{本文研究内容与贡献}
针对当前 AI 系统在多层次架构中逐渐显现的供应链安全风险，本文从系统整体视角出发，对 AI 系统的软件应用层、模型框架层以及硬件加速层三个关键层级开展了系统性的安全分析。针对当前 AI 系统在多层次架构中逐渐显现的供应链安全风险，本文从系统整体视角出发，对 AI 系统的软件应用层、模型框架层以及硬件加速层三个关键层级开展了系统性的安全分析。通过对上述三个层级的深入研究，本文不仅弥补了现有工作在跨层级系统性分析方面的不足，还在每一层级中发现了此前尚未被充分认识的安全问题，并提出了具有实践意义的分析方法与防护思路，从而构建了一套覆盖 AI 系统全栈的安全研究体系，为理解和保障 AI 系统在真实部署环境下的安全性提供了重要的理论与实践参考。

具体而言，本文的研究内容和主要贡献可概括为以下三个层级。

\textbf{软件应用层：细粒度模块级依赖冲突安全分析。}
在软件应用层，现有研究主要关注依赖配置错误、已知漏洞传播以及漏洞检测等安全问题，且多以软件包为分析粒度。然而，在以 Python 为代表的 AI 软件生态中，大量项目由复杂的模块级依赖关系构成，不同软件包在安装与运行阶段可能产生细粒度的模块冲突行为，其安全影响尚未得到系统性研究。

为填补这一研究空白，本文围绕 Python 生态中的模块级依赖冲突问题开展了系统研究。首先，本文从 GitHub、Stack Overflow 等开发者社区出发，采用滚雪球式的数据收集方法，系统整理并分析了大量与模块冲突相关的真实事件，对模块冲突的触发场景、表现形式及潜在影响进行了实证研究总结~\cite{github-website,stackoverflow-website}。在此基础上，本文进一步对整个 Python 开源生态 (The Python Package Index, PyPI) 生态中的全部软件包进行了大规模模块收集与分析，系统识别可能存在模块冲突风险的软件包及其潜在影响范围。最后，本文以真实世界的 AI 项目为对象，对 GitHub 上广泛使用的热门 AI 项目进行了深入分析，评估模块冲突问题在实际 AI 软件构建与运行过程中的安全影响。

围绕软件应用层的模块级依赖冲突问题，本文的主要贡献包括：

\begin{itemize}
\item \textbf{新攻击面：} 本文揭示了一种软件应用层的新型攻击面——模块替换攻击。该攻击利用 Python 软件包在安装阶段将不同包的模块部署至同一默认路径的机制，通过构造同名模块引发冲突，从而干扰 AI 软件的正常执行，甚至实现任意代码执行等安全威胁。
\item \textbf{新技术：} 为支撑大规模生态分析，本文提出了两项关键技术：\code{InstSimulator} 与 \code{EnvResolution}。\code{InstSimulator} 通过 AST 解析与动态安装模拟相结合的方式，解决了软件包源代码结构与实际安装后模块布局不一致的问题；\code{EnvResolution} 则通过环境语义建模与免下载依赖解析机制，有效提升了依赖图构建的准确性与分析效率。
\item \textbf{大规模实证研究：} 基于上述技术，本文实现了 \code{ModuleGuard} 框架，对 PyPI 生态中 43 万余个软件包、420 万余个版本进行了系统性的模块依赖与冲突分析，系统总结了模块冲突问题的成因、特征及其在生态中的分布情况。
\item \textbf{真实世界影响分析：} 利用 \code{ModuleGuard}，本文进一步分析了 GitHub 上 3,711 个真实热门 AI 项目（涵盖 93,487 个版本标签），发现其中 108 个高星项目存在实际的模块冲突风险，并已向相关开发者报告问题并提供修复建议。
% \item \textbf{新技术：} 为了对整个 Python 生态进行大规模实证分析，本研究提出了两个新技术：\code{InstSimulator} 和 \code{EnvResolution}。\code{InstSimulator} 解决了 Python 软件包中安装后的模块与软件包安装前内部文件结构不一致的问题，该技术利用 AST 解析和动态安装模拟的方式来获得 Python 软件包安装后的模块路径。\code{EnvResolution} 技术解决了 Python 软件包在安装过程中会由于安装环境导致的依赖解析不准确问题，以及传统的以 \code{pip} 解析依赖图速度慢，并不适配大规模分析研究的问题，该技术采用了一种免下载的处理模式，并且采用环境收集语义获取用户本地的会影响依赖图的环境信息，并在解析过程中作为依赖图解析的变量进行处理。
% \item \textbf{大规模实证研究：} 基于以上两个新技术，本研究提出了 \code{ModuleGuard} 框架，用以大规模地对整个 PyPI 生态 434 823 万个软件包的 420 万个版本进行系统性地依赖图和模块冲突分析，不仅分析了模块冲突问题以及模块替换攻击的成因，总结了这些问题的特征，并且分析了可能遭受模块冲突风险的软件包数量。
% \item \textbf{真实世界影响报告：} 基于 \code{ModuleGuard} 框架，本研究对 GitHub 上 3 711个 真实热门的 AI 项目，包括了 93 487 个 tag， 进行了系统的依赖分析和模块冲突分析，发现108个的高星 AI 项目都存在模块冲突的风险，本研究向相关开发者汇报了相关冲突风险，并提供了指导修复意见。
\end{itemize}

本部分研究的详细内容见本文第三章，相关代码开源于网站 \url{https://sites.google.com/view/moduleguard}，研究成果发表于 ICSE (CCF-A 类) 国际软件工程顶级学术会议。

\textbf{模型框架层：基于合法 API 能力滥用的函数级攻击范式。}
在模型框架层，现有研究通常将恶意模型视为参数投毒或模型逻辑篡改问题，关注模型输出异常或性能退化等结果性表现。尽管已有少量工作尝试将模型本身作为恶意载体，但其攻击方式往往依赖不安全序列化或显式代码注入，不仅容易影响模型精度，也较易被现有检测工具发现。

随着深度学习框架功能的不断扩展，其所提供的大量高权限、通用型 API 已逐渐具备超出模型计算本身的系统交互能力。然而，这类“合法 API 所隐含的安全风险”在现有研究中尚未得到系统性分析。针对这一问题，本文以 TensorFlow 框架为研究对象，系统分析了其框架 API 的持久化机制及能力边界，提出了 TensorAbuse 攻击模型，揭示攻击者如何在不依赖传统漏洞的情况下，仅通过滥用框架提供的正常 API，将恶意行为嵌入 AI 模型之中，从而在模型运行阶段触发系统级攻击。

围绕模型框架层的 API 能力滥用问题，本文的主要贡献包括：
\begin{itemize}
\item \textbf{新攻击面：} 本文提出了模型框架层的新型攻击范式——TensorAbuse。该攻击利用深度学习框架 API 自身具备的文件访问与网络通信能力，在不影响模型精度的前提下，实现任意代码执行、文件窃取等系统级攻击行为。
\item \textbf{新技术：} 本文提出了 \code{PersistExt} 与 \code{CapAnalysis} 两项关键技术，用于自动化提取并分析框架 API 的潜在高风险能力。\code{PersistExt} 结合静态分析与启发式规则，系统提取可被持久化至模型中的 API 及其跨语言调用链；\code{CapAnalysis} 则借助大语言模型能力，对 API 的潜在系统交互能力进行自动化判定。
\item \textbf{真实世界影响验证：} 基于上述技术，本文从 TensorFlow 中识别出 1,083 个可嵌入模型的 API，并进一步筛选出 31 个可用于构造恶意行为的高风险 API，构造了五类攻击原语与四类完整攻击模型，成功绕过 Hugging Face、ModelScan 等主流检测工具，并获得厂商认可与安全奖励。本研究向这些厂商提供相关利用的详细代码，帮助他们构建能力更强的模型扫描工具，同时收获了 1,650 美元的奖励。
\item \textbf{恶意模型检测工具：}在攻击分析的基础上，本文实现并开源了一套恶意模型检测工具，可对 TensorAbuse 及相关隐蔽嵌入方式进行自动化检测并生成分析报告。
\end{itemize}

本部分研究的详细内容见本文第四章，相关代码开源于 \url{https://github.com/ZJU-SEC/TensorAbuse}，研究成果发表于 IEEE S\&P (CCF-A 类) 网络安全领域国际顶级学术会议。

\textbf{硬件加速层：GPU 内存地址随机化安全机制分析与跨设备攻击。}
在硬件加速层，GPU 等专用计算设备已成为 AI 系统不可或缺的基础设施。尽管已有大量研究对 GPU 架构、侧信道攻击以及驱动与固件漏洞进行了深入分析，但现有工作大多将 GPU 视为相对独立的计算单元，主要关注 GPU 内部的安全问题，而缺乏对 GPU 与 CPU 等异构计算设备之间安全关联性的系统研究，尤其尚未从供应链视角分析 GPU 侧安全机制失效对整个 AI 系统的潜在影响。

针对这一不足，本文从 GPU 侧防护机制的安全性出发，重点分析了 NVIDIA GPU 中地址随机化 (Address Space Layout Randomization, ASLR) 机制的设计假设与实际实现之间的差异。本文发现，GPU ASLR 在实现层面存在非随机区域、粗粒度随机化以及地址相关性等问题，使得攻击者仅需利用 GPU 侧的简单内存安全漏洞（如越界读），即可泄露 GPU 内部的敏感地址信息。进一步地，本文系统分析了 GPU 与 CPU 在统一虚拟内存和物理内存映射机制下的交互关系，揭示了 GPU 地址泄露如何被用于逐步推断 CPU 侧地址空间布局，最终首次实现了从 GPU 出发影响 CPU 地址随机化安全性的跨设备攻击路径。
% 在硬件加速层，GPU 等专用计算设备已成为 AI 系统不可或缺的供应链基础设施。尽管已有大量工作对 GPU 架构，侧信道攻击，以及 GPU 等计算设备的驱动和固件漏洞进行了深入研究，然而这些研究仅仅针对单一的计算设备，并没有分析跨设备的安全性，尤其是从 GPU 等计算设备攻击 CPU，从而影响 AI 系统的上层的影响。为了弥补这一空白，本研究从 GPU 侧的不健全的防护机制出发，尤其是对 GPU 地址随机化机制本身的安全性，系统分析了其设计假设与实际实现之间的差异，揭示了攻击者如何通过 GPU 侧的一个简单漏洞即可实现的信息泄漏，并逐步推断地址布局，然后进一步对 CPU 侧地址空间产生影响，首次实现了跨设备的攻击。

围绕硬件加速层的 GPU 地址随机化机制，本文的主要贡献包括：
\begin{itemize}
\item \textbf{新攻击面：} 本文揭示了一种硬件加速层的新型攻击面。攻击者可利用 NVIDIA GPU 中地址随机化机制的弱点，通过 GPU 侧的 OOB 漏洞泄露关键地址信息，并进一步借助 GPU 与 CPU 地址空间之间的相关性，推断 CPU 侧 glibc 等关键段的地址布局，从而削弱 CPU 侧 ASLR 的安全性。
\item \textbf{新技术：} 本文提出了 \code{FlagProbe} 与 \code{AnchorTrace} 两项关键逆向分析技术，用于在缺乏文档支持的黑盒环境下恢复 NVIDIA GPU 虚拟地址空间的语义结构。 \code{FlagProbe} 通过启发式标志插入与访问行为分析，恢复不同 GPU 内存区域的语义；\code{AnchorTrace} 则利用系统常量内存作为锚点，递归追踪稳定指针链，从而高效收集 GPU 侧随机化地址信息。这两项技术共同构成了一套可复用的 GPU 内存布局分析框架。
\item \textbf{新发现：} 本文系统揭示了多个此前未被公开披露的 GPU 内存布局特性：多个 CUDA 进程共享关键 GPU 内存区域；GPU 与 CPU 在不同权限模型下映射相同的物理内存页；GPU 堆内存完全缺乏随机化，其余内存区域采用统一的粗粒度随机偏移；此外，部分 CPU 地址空间（如 glibc）与 GPU 地址空间之间仅存在极低熵差异。这些特性共同削弱了 GPU ASLR 的防护效果，并为跨处理器攻击提供了现实基础。
\item \textbf{跨设备攻击验证：} 基于上述发现与技术，本文发现了一个 GPU 缺陷。基于该缺陷，本文构建并验证了一条完整的跨设备攻击路径，展示了攻击者如何从 GPU 侧漏洞出发，逐步破坏 GPU ASLR，并进一步推断 CPU 地址随机化信息，从而对整个 AI 系统的安全性产生连锁影响。相关缺陷已经汇报给 NVIDIA 官方，并已经在 570 版本驱动中修复。
% \item \textbf{新技术：} 本研究提出了两个新技术：\code{FlagProbe} 和 \code{AnchorTrace}，用以逆向黑盒的 NVIDIA GPU 上虚拟地址空间以及分析其语义。\code{FlagProbe} 是一项利用启发式规则匹配的逆向技术，其将 GPU 侧虚拟地址空间中不同段的语义信息恢复出来。\code{AnchorTrace} 利用特殊的指针追踪技术和构造的内存块图，寻找可靠指针链，从而快速收集 GPU 侧随机化的地址信息。利用这两项技术，本研究提出了 GPU 分析框架，逆向出了黑盒的 NVIDIA GPU 侧的虚拟地址空间，并且分析出了其实现的地址随机化保护机制的细节。
% \item \textbf{新发现：} 本研究发现，对于 GPU 内存布局，所有 CUDA 进程共享两个较大的 GPU 内存区域，并将一个 CPU 内核空间页面映射到用户特权的 GPU 进程的内存布局。此外，CPU 和 GPU 映射了相同的物理内存页到各自的虚拟地址空间，但权限不同：GPU 具有 RWX 访问权限，而 CPU 受限于 RW 访问权限。这些 CUDA 进程间的共享内存区域为隐蔽通道攻击提供了便利，而具有不同权限的 GPU 和 CPU 内存共享使得潜在的跨处理器攻击成为可能，例如从 CPU 注入 GPU 可执行代码或在 GPU 上生成恶意数据以危及 CPU。我们还观察到，即使启用了 GPU ASLR， GPU 堆仍然完全是非随机的。大多数其他 GPU 内存区域(包括 .text 和 CUDA 库)以相同的 随机偏移量运行。此外，某些 CPU 内存区域（例如 glibc）表现出与 GPU 区域相关的 随机化偏移量，它们的差异只有 7 位熵。这种非随机的堆和粗粒度的随机化破坏了 GPU ASLR 的有效性；ASLR 偏移量相关性进一步允许攻击者从GPU推断出CPU的ASLR偏移量。
\end{itemize}

本部分研究的详细内容见本文第五章，相关代码开源于 \url{https://github.com/ZJU-SEC/NvidiaASLR}，研究成果发表于 IEEE S\&P (CCF-A 类) 网络安全领域国际顶级学术会议。

\section{本文组织与章节安排}


% 我们可以用includegraphics来插入现有的jpg等格式的图片，
% 如\autoref{fig:zju-logo}所示。

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=.3\linewidth]{logo/zju}
%     \caption{\label{fig:zju-logo}浙江大学LOGO}
% \end{figure}


% \subsection{小节标题}


% \par 如\autoref{tab:sample}所示，这是一张自动调节列宽的表格。

% \begin{table}[htbp]
%     \caption{\label{tab:sample}自动调节列宽的表格}
%     \begin{tabularx}{\linewidth}{c|X<{\centering}}
%         \hline
%         第一列 & 第二列 \\ \hline
%         xxx & xxx \\ \hline
%         xxx & xxx \\ \hline
%         xxx & xxx \\ \hline
%     \end{tabularx}
% \end{table}


% \par 如\autoref{equ:sample}，这是一个公式

% \begin{equation}
%     \label{equ:sample}
%     A=\overbrace{(a+b+c)+\underbrace{i(d+e+f)}_{\text{虚数}}}^{\text{复数}}
% \end{equation}

% \chapter{另一章}


% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=.3\linewidth]{example-image-a}
%     \caption{\label{fig:fig-placeholder}图片占位符}
% \end{figure}

% \chapter{再一章}

% \par 如\autoref{alg:sample}，这是一个算法

% \begin{algorithm}[H]
%     \begin{algorithmic} % enter the algorithmic environment
%         \REQUIRE $n \geq 0 \vee x \neq 0$
%         \ENSURE $y = x^n$
%         \STATE $y \Leftarrow 1$
%         \IF{$n < 0$}
%             \STATE $X \Leftarrow 1 / x$
%             \STATE $N \Leftarrow -n$
%         \ELSE
%             \STATE $X \Leftarrow x$
%             \STATE $N \Leftarrow n$
%         \ENDIF
%         \WHILE{$N \neq 0$}
%             \IF{$N$ is even}
%                 \STATE $X \Leftarrow X \times X$
%                 \STATE $N \Leftarrow N / 2$
%             \ELSE[$N$ is odd]
%                 \STATE $y \Leftarrow y \times X$
%                 \STATE $N \Leftarrow N - 1$
%             \ENDIF
%         \ENDWHILE
%     \end{algorithmic}
%     \caption{\label{alg:sample}算法样例}
% \end{algorithm}